INFO - 12/16/23 23:01:32 - 0:00:00 - ============ Initialized logger ============
INFO - 12/16/23 23:01:32 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/16/23 23:01:32 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/16/23 23:01:32 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/16/23 23:01:32 - 0:00:00 - ============ Data summary ============
INFO - 12/16/23 23:01:32 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/16/23 23:01:32 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/16/23 23:01:32 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/16/23 23:01:32 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/16/23 23:01:32 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/16/23 23:01:32 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:01:32 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:01:32 - 0:00:00 - Para Data          -       5703
INFO - 12/16/23 23:01:32 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/16/23 23:01:32 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/16/23 23:01:32 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:01:32 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/16/23 23:02:15 - 0:00:00 - ============ Initialized logger ============
INFO - 12/16/23 23:02:15 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/16/23 23:02:15 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/16/23 23:02:15 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/16/23 23:02:15 - 0:00:00 - ============ Data summary ============
INFO - 12/16/23 23:02:15 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/16/23 23:02:15 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/16/23 23:02:15 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/16/23 23:02:15 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/16/23 23:02:15 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/16/23 23:02:15 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:02:15 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:02:15 - 0:00:00 - Para Data          -       5703
INFO - 12/16/23 23:02:15 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/16/23 23:02:15 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/16/23 23:02:15 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/16/23 23:02:15 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:19:44 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:19:44 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:19:44 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:19:44 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:19:44 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:19:44 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:19:44 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:19:44 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:19:44 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:19:44 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:19:44 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:19:44 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:19:44 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:19:44 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:19:44 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:19:44 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:19:44 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:21:43 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:21:43 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:21:43 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:21:43 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:21:43 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:21:43 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:21:43 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:21:43 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:21:43 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:21:43 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:21:43 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:21:43 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:21:43 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:21:43 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:21:43 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:21:43 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:21:43 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:24:19 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:24:19 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:24:19 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:24:19 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:24:19 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:24:19 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:24:19 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:24:19 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:24:19 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:24:19 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:24:19 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:24:19 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:24:19 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:24:19 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:24:19 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:24:19 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:24:19 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:24:21 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:24:21 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:24:21 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:24:21 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:31:45 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:31:45 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:31:45 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:31:45 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:31:45 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:31:45 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:31:45 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:31:45 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:31:45 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:31:45 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:31:45 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:31:45 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:31:45 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:31:45 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:31:45 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:31:45 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:31:45 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:31:46 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:31:46 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:31:46 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:31:46 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:33:14 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:33:14 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:33:14 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:33:14 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:33:14 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:33:14 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:33:14 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:33:14 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:33:14 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:33:14 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:33:14 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:33:14 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:33:14 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:33:14 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:33:14 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:33:14 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:33:14 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:33:15 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:33:15 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:33:15 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:33:15 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:35:09 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:35:09 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:35:09 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:35:09 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:35:09 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:35:09 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:35:09 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:35:09 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:35:09 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:35:09 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:35:09 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:35:09 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:35:09 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:35:09 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:35:09 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:35:09 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:35:09 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:35:10 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:35:10 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:35:10 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:35:10 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:36:22 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:36:22 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:36:22 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:36:22 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:36:22 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:36:22 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:36:22 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:36:22 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:36:22 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:36:22 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:36:22 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:36:22 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:36:22 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:36:22 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:36:22 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:36:22 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:36:22 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:36:23 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:36:23 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:36:23 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:36:23 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:36:24 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:36:24 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:36:24 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 15:36:28 - 0:00:06 -      10 -   31.56 sent/s -  1391.32 words/s - cs:  0.5556 (coef=1.0000) || sm:  7.6147 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:31 - 0:00:09 -      20 -   37.70 sent/s -  1833.27 words/s - cs:  0.3263 (coef=1.0000) || sm:  6.1272 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:34 - 0:00:12 -      30 -   40.77 sent/s -  1546.93 words/s - cs:  0.3802 (coef=1.0000) || sm:  6.0146 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:37 - 0:00:16 -      40 -   33.20 sent/s -  1642.31 words/s - cs:  0.2785 (coef=1.0000) || sm:  6.2856 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:41 - 0:00:19 -      50 -   34.97 sent/s -  1632.82 words/s - cs:  0.2532 (coef=1.0000) || sm:  6.1341 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:44 - 0:00:22 -      60 -   37.08 sent/s -  1771.32 words/s - cs:  0.3045 (coef=1.0000) || sm:  6.1760 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:48 - 0:00:26 -      70 -   33.96 sent/s -  1480.55 words/s - cs:  0.3239 (coef=1.0000) || sm:  6.1575 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:51 - 0:00:29 -      80 -   38.83 sent/s -  1806.17 words/s - cs:  0.2449 (coef=1.0000) || sm:  5.8727 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:54 - 0:00:32 -      90 -   38.79 sent/s -  1580.38 words/s - cs:  0.3553 (coef=1.0000) || sm:  5.9190 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:36:57 - 0:00:35 -     100 -   37.92 sent/s -  1715.70 words/s - cs:  0.2539 (coef=1.0000) || sm:  5.9125 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:00 - 0:00:38 -     110 -   39.09 sent/s -  1934.93 words/s - cs:  0.3573 (coef=1.0000) || sm:  5.9526 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:03 - 0:00:42 -     120 -   37.68 sent/s -  1925.44 words/s - cs:  0.2680 (coef=1.0000) || sm:  6.1703 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:06 - 0:00:45 -     130 -   38.48 sent/s -  1737.88 words/s - cs:  0.2468 (coef=1.0000) || sm:  5.9246 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:09 - 0:00:48 -     140 -   39.95 sent/s -  1473.34 words/s - cs:  0.4604 (coef=1.0000) || sm:  5.8204 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:13 - 0:00:51 -     150 -   36.56 sent/s -  1505.93 words/s - cs:  0.3200 (coef=1.0000) || sm:  6.0254 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:16 - 0:00:54 -     160 -   37.68 sent/s -  1441.81 words/s - cs:  0.4040 (coef=1.0000) || sm:  6.0889 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:19 - 0:00:57 -     170 -   36.44 sent/s -  1963.25 words/s - cs:  0.1958 (coef=1.0000) || sm:  5.9427 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:22 - 0:01:01 -     180 -   37.16 sent/s -  1751.55 words/s - cs:  0.2781 (coef=1.0000) || sm:  6.0303 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:25 - 0:01:04 -     190 -   39.13 sent/s -  1454.57 words/s - cs:  0.3523 (coef=1.0000) || sm:  5.9065 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:28 - 0:01:07 -     200 -   38.51 sent/s -  1741.18 words/s - cs:  0.3666 (coef=1.0000) || sm:  5.9690 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:32 - 0:01:10 -     210 -   35.83 sent/s -  1399.74 words/s - cs:  0.3995 (coef=1.0000) || sm:  6.0168 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:35 - 0:01:13 -     220 -   36.72 sent/s -  1795.70 words/s - cs:  0.3098 (coef=1.0000) || sm:  6.0872 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:38 - 0:01:17 -     230 -   35.82 sent/s -  1619.88 words/s - cs:  0.2869 (coef=1.0000) || sm:  5.8606 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:42 - 0:01:20 -     240 -   32.58 sent/s -  1525.72 words/s - cs:  0.4024 (coef=1.0000) || sm:  6.3057 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:45 - 0:01:24 -     250 -   36.03 sent/s -  1466.74 words/s - cs:  0.3408 (coef=1.0000) || sm:  6.0724 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:48 - 0:01:27 -     260 -   39.25 sent/s -  1786.69 words/s - cs:  0.3579 (coef=1.0000) || sm:  5.7834 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:37:52 - 0:01:30 -     270 -   36.11 sent/s -  1538.39 words/s - cs:  0.2719 (coef=1.0000) || sm:  6.0217 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:42:39 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:42:39 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:42:39 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:42:39 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:42:39 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:42:39 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:42:39 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:42:39 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:42:39 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:42:39 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:42:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:42:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:42:39 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:42:39 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:42:39 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:42:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:42:39 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:42:40 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:42:40 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:42:40 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:42:40 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:42:41 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:42:41 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:42:41 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 15:42:44 - 0:00:05 -      10 -   32.90 sent/s -  1513.28 words/s - cs:  0.5779 (coef=1.0000) || sm:  7.4017 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:42:47 - 0:00:09 -      20 -   37.46 sent/s -  1815.59 words/s - cs:  0.3568 (coef=1.0000) || sm:  6.0369 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:42:51 - 0:00:12 -      30 -   36.99 sent/s -  1763.66 words/s - cs:  0.3371 (coef=1.0000) || sm:  6.2185 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:42:54 - 0:00:15 -      40 -   33.58 sent/s -  1757.76 words/s - cs:  0.3352 (coef=1.0000) || sm:  6.2573 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:42:57 - 0:00:18 -      50 -   38.46 sent/s -  1657.70 words/s - cs:  0.3159 (coef=1.0000) || sm:  6.0497 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:43:01 - 0:00:22 -      60 -   38.66 sent/s -  1629.34 words/s - cs:  0.4393 (coef=1.0000) || sm:  6.0739 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:43:04 - 0:00:25 -      70 -   33.58 sent/s -  1566.99 words/s - cs:  0.3996 (coef=1.0000) || sm:  6.1002 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:43:07 - 0:00:28 -      80 -   36.73 sent/s -  1644.08 words/s - cs:  0.3710 (coef=1.0000) || sm:  6.0277 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:44:53 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:44:53 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:44:53 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:44:53 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:44:53 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:44:53 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:44:53 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:44:53 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:44:53 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:44:53 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:44:53 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:44:53 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:44:53 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:44:53 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:44:53 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:44:53 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:44:53 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:44:54 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:44:54 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:44:54 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:44:54 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:44:55 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:44:55 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:49:06 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:49:06 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:49:06 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:49:06 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:49:06 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:49:06 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:49:06 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:49:06 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:49:06 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:49:06 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:49:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:49:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:49:06 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:49:06 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:49:06 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:49:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:49:06 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:49:07 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:49:07 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:49:07 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:49:07 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:49:07 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:49:07 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:50:16 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:50:16 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:50:16 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:50:16 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:50:16 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:50:16 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:50:16 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:50:16 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:50:16 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:50:16 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:50:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:50:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:50:16 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:50:16 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:50:16 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:50:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:50:16 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:50:18 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:50:18 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:50:18 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:50:18 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:50:18 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:50:18 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:50:18 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 15:50:22 - 0:00:05 -      10 -   33.43 sent/s -  1346.61 words/s - cs:  1.0863 (coef=1.0000) || sm:  7.4205 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:25 - 0:00:09 -      20 -   33.01 sent/s -  1354.11 words/s - cs:  0.5392 (coef=1.0000) || sm:  6.2858 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:28 - 0:00:12 -      30 -   38.14 sent/s -  1677.40 words/s - cs:  0.3221 (coef=1.0000) || sm:  5.9348 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:32 - 0:00:15 -      40 -   37.51 sent/s -  1617.70 words/s - cs:  0.3814 (coef=1.0000) || sm:  5.9389 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:35 - 0:00:19 -      50 -   35.59 sent/s -  1749.70 words/s - cs:  0.2538 (coef=1.0000) || sm:  5.9442 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:38 - 0:00:22 -      60 -   36.43 sent/s -  1459.54 words/s - cs:  0.3623 (coef=1.0000) || sm:  6.0288 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:42 - 0:00:25 -      70 -   35.06 sent/s -  1602.43 words/s - cs:  0.3773 (coef=1.0000) || sm:  6.0859 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:45 - 0:00:29 -      80 -   34.57 sent/s -  1572.14 words/s - cs:  0.4717 (coef=1.0000) || sm:  6.1499 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:49 - 0:00:32 -      90 -   35.08 sent/s -  1717.36 words/s - cs:  0.3293 (coef=1.0000) || sm:  6.0874 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:52 - 0:00:36 -     100 -   32.54 sent/s -  1654.83 words/s - cs:  0.2707 (coef=1.0000) || sm:  6.0255 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:56 - 0:00:39 -     110 -   36.20 sent/s -  1716.66 words/s - cs:  0.2408 (coef=1.0000) || sm:  6.1499 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:50:59 - 0:00:43 -     120 -   33.76 sent/s -  1802.03 words/s - cs:  0.3250 (coef=1.0000) || sm:  6.2875 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:02 - 0:00:46 -     130 -   37.39 sent/s -  1671.06 words/s - cs:  0.2930 (coef=1.0000) || sm:  5.8144 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:06 - 0:00:49 -     140 -   37.64 sent/s -  1754.67 words/s - cs:  0.2445 (coef=1.0000) || sm:  5.9977 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:09 - 0:00:53 -     150 -   32.52 sent/s -  1378.10 words/s - cs:  0.4315 (coef=1.0000) || sm:  6.1578 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:13 - 0:00:56 -     160 -   36.79 sent/s -  1582.69 words/s - cs:  0.3053 (coef=1.0000) || sm:  6.0967 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:16 - 0:00:59 -     170 -   37.59 sent/s -  1997.13 words/s - cs:  0.2569 (coef=1.0000) || sm:  5.8015 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:19 - 0:01:02 -     180 -   38.45 sent/s -  1512.87 words/s - cs:  0.3300 (coef=1.0000) || sm:  5.6296 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:22 - 0:01:06 -     190 -   34.52 sent/s -  1430.45 words/s - cs:  0.3405 (coef=1.0000) || sm:  5.7970 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:26 - 0:01:09 -     200 -   35.75 sent/s -  1341.86 words/s - cs:  0.3922 (coef=1.0000) || sm:  5.8327 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:29 - 0:01:13 -     210 -   35.71 sent/s -  1633.00 words/s - cs:  0.3950 (coef=1.0000) || sm:  5.9212 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:32 - 0:01:16 -     220 -   36.73 sent/s -  1792.12 words/s - cs:  0.3004 (coef=1.0000) || sm:  6.0140 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:36 - 0:01:19 -     230 -   33.75 sent/s -  1618.25 words/s - cs:  0.2652 (coef=1.0000) || sm:  6.0303 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:39 - 0:01:23 -     240 -   37.53 sent/s -  1611.62 words/s - cs:  0.3080 (coef=1.0000) || sm:  5.8328 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:42 - 0:01:26 -     250 -   37.36 sent/s -  1679.65 words/s - cs:  0.3177 (coef=1.0000) || sm:  5.8750 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:46 - 0:01:29 -     260 -   36.26 sent/s -  1670.86 words/s - cs:  0.2544 (coef=1.0000) || sm:  5.9391 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:49 - 0:01:33 -     270 -   34.96 sent/s -  1686.34 words/s - cs:  0.3071 (coef=1.0000) || sm:  5.9349 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:52 - 0:01:36 -     280 -   35.20 sent/s -  1320.14 words/s - cs:  0.3837 (coef=1.0000) || sm:  5.8351 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:51:56 - 0:01:39 -     290 -   34.53 sent/s -  1742.09 words/s - cs:  0.3259 (coef=1.0000) || sm:  6.0677 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:00 - 0:01:44 -     300 -   29.12 sent/s -  1518.33 words/s - cs:  0.3415 (coef=1.0000) || sm:  6.2192 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:03 - 0:01:47 -     310 -   37.47 sent/s -  1552.36 words/s - cs:  0.2976 (coef=1.0000) || sm:  5.8475 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:06 - 0:01:50 -     320 -   37.29 sent/s -  1502.84 words/s - cs:  0.3746 (coef=1.0000) || sm:  5.6146 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:10 - 0:01:53 -     330 -   35.73 sent/s -  1639.35 words/s - cs:  0.2802 (coef=1.0000) || sm:  5.9368 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:13 - 0:01:57 -     340 -   38.02 sent/s -  1923.79 words/s - cs:  0.3109 (coef=1.0000) || sm:  5.7662 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:16 - 0:02:00 -     350 -   37.21 sent/s -  1742.08 words/s - cs:  0.2257 (coef=1.0000) || sm:  5.7425 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:20 - 0:02:03 -     360 -   32.32 sent/s -  1660.29 words/s - cs:  0.2437 (coef=1.0000) || sm:  6.0531 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:23 - 0:02:07 -     370 -   36.89 sent/s -  1427.14 words/s - cs:  0.3536 (coef=1.0000) || sm:  5.8482 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:27 - 0:02:10 -     380 -   35.44 sent/s -  1536.04 words/s - cs:  0.3321 (coef=1.0000) || sm:  5.9479 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:30 - 0:02:14 -     390 -   33.56 sent/s -  1656.81 words/s - cs:  0.2508 (coef=1.0000) || sm:  6.0165 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:34 - 0:02:17 -     400 -   34.80 sent/s -  1386.77 words/s - cs:  0.4002 (coef=1.0000) || sm:  5.8927 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:38 - 0:02:21 -     410 -   30.42 sent/s -  1751.59 words/s - cs:  0.2828 (coef=1.0000) || sm:  5.9358 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:41 - 0:02:24 -     420 -   37.49 sent/s -  1634.10 words/s - cs:  0.4897 (coef=1.0000) || sm:  5.9083 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:44 - 0:02:27 -     430 -   38.19 sent/s -  1591.27 words/s - cs:  0.3874 (coef=1.0000) || sm:  5.6883 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:47 - 0:02:30 -     440 -   40.74 sent/s -  1608.60 words/s - cs:  0.3539 (coef=1.0000) || sm:  5.7688 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:50 - 0:02:34 -     450 -   36.97 sent/s -  1725.86 words/s - cs:  0.2787 (coef=1.0000) || sm:  5.9501 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:53 - 0:02:37 -     460 -   38.38 sent/s -  1501.58 words/s - cs:  0.3706 (coef=1.0000) || sm:  5.8267 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:52:56 - 0:02:40 -     470 -   37.92 sent/s -  1532.01 words/s - cs:  0.3543 (coef=1.0000) || sm:  5.9548 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:53:00 - 0:02:43 -     480 -   36.28 sent/s -  2048.30 words/s - cs:  0.2367 (coef=1.0000) || sm:  6.1231 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:53:03 - 0:02:46 -     490 -   36.58 sent/s -  1745.16 words/s - cs:  0.2793 (coef=1.0000) || sm:  6.0135 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:53:06 - 0:02:50 -     500 -   34.01 sent/s -  1416.21 words/s - cs:  0.3486 (coef=1.0000) || sm:  5.9685 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 15:57:17 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 15:57:17 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 15:57:17 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 15:57:17 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 15:57:17 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 15:57:17 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:57:17 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 15:57:17 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 15:57:17 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 15:57:17 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 15:57:17 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:57:17 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:57:17 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 15:57:17 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 15:57:17 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 15:57:17 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 15:57:17 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 15:57:18 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(800, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 15:57:18 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 15:57:18 - 0:00:01 - Number of parameters (encoder): 10972801
INFO - 12/20/23 15:57:18 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 15:57:19 - 0:00:01 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 15:57:19 - 0:00:01 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 15:57:19 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 16:00:16 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 16:00:16 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 1
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 16:00:16 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 16:00:16 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 1 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 16:00:16 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 16:00:16 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 16:00:16 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 16:00:16 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 16:00:16 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 16:00:16 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 16:00:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 16:00:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 16:00:16 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 16:00:16 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 16:00:16 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 16:00:16 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 16:00:16 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 16:00:17 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(1500, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 16:00:17 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 16:00:17 - 0:00:01 - Number of parameters (encoder): 11331201
INFO - 12/20/23 16:00:17 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 16:00:17 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 16:00:17 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 16:00:17 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 16:00:21 - 0:00:05 -      10 -   31.96 sent/s -  1310.25 words/s - cs:  0.7028 (coef=1.0000) || sm:  7.4293 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:24 - 0:00:09 -      20 -   36.74 sent/s -  1587.91 words/s - cs:  0.3704 (coef=1.0000) || sm:  6.2054 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:27 - 0:00:12 -      30 -   38.02 sent/s -  1779.19 words/s - cs:  0.3806 (coef=1.0000) || sm:  6.1075 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:31 - 0:00:15 -      40 -   33.21 sent/s -  1526.19 words/s - cs:  0.3984 (coef=1.0000) || sm:  6.1267 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:34 - 0:00:19 -      50 -   35.09 sent/s -  1543.06 words/s - cs:  0.3286 (coef=1.0000) || sm:  6.0755 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:39 - 0:00:24 -      60 -   25.58 sent/s -  1068.61 words/s - cs:  0.4283 (coef=1.0000) || sm:  6.1132 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:43 - 0:00:27 -      70 -   32.31 sent/s -  1650.33 words/s - cs:  0.3206 (coef=1.0000) || sm:  6.1511 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:46 - 0:00:31 -      80 -   34.97 sent/s -  1544.89 words/s - cs:  0.3571 (coef=1.0000) || sm:  6.2711 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:50 - 0:00:35 -      90 -   29.42 sent/s -  2136.18 words/s - cs:  0.4030 (coef=1.0000) || sm:  6.0297 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:54 - 0:00:38 -     100 -   35.64 sent/s -  1834.03 words/s - cs:  0.2976 (coef=1.0000) || sm:  5.9305 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:00:57 - 0:00:41 -     110 -   38.65 sent/s -  1545.70 words/s - cs:  0.3616 (coef=1.0000) || sm:  5.9883 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:00 - 0:00:44 -     120 -   38.32 sent/s -  1652.34 words/s - cs:  0.3423 (coef=1.0000) || sm:  5.9074 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:03 - 0:00:48 -     130 -   36.57 sent/s -  1586.72 words/s - cs:  0.2734 (coef=1.0000) || sm:  5.8817 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:06 - 0:00:51 -     140 -   38.09 sent/s -  1646.17 words/s - cs:  0.3406 (coef=1.0000) || sm:  5.9535 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:10 - 0:00:54 -     150 -   35.26 sent/s -  1456.84 words/s - cs:  0.3124 (coef=1.0000) || sm:  5.9397 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:13 - 0:00:58 -     160 -   34.54 sent/s -  1853.83 words/s - cs:  0.3022 (coef=1.0000) || sm:  6.0154 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:16 - 0:01:01 -     170 -   36.26 sent/s -  1800.67 words/s - cs:  0.2912 (coef=1.0000) || sm:  6.0128 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:20 - 0:01:04 -     180 -   35.85 sent/s -  1710.69 words/s - cs:  0.4094 (coef=1.0000) || sm:  5.9425 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:23 - 0:01:08 -     190 -   36.52 sent/s -  1587.33 words/s - cs:  0.3229 (coef=1.0000) || sm:  5.7171 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:26 - 0:01:11 -     200 -   36.99 sent/s -  1544.81 words/s - cs:  0.3455 (coef=1.0000) || sm:  5.8956 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:30 - 0:01:14 -     210 -   35.55 sent/s -  1661.70 words/s - cs:  0.3229 (coef=1.0000) || sm:  6.1453 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:33 - 0:01:18 -     220 -   34.23 sent/s -  1740.62 words/s - cs:  0.2575 (coef=1.0000) || sm:  6.0286 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:37 - 0:01:21 -     230 -   33.28 sent/s -  1585.24 words/s - cs:  0.2875 (coef=1.0000) || sm:  6.0229 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:40 - 0:01:25 -     240 -   32.50 sent/s -  1576.27 words/s - cs:  0.2608 (coef=1.0000) || sm:  5.9424 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:44 - 0:01:29 -     250 -   33.03 sent/s -  1561.85 words/s - cs:  0.2324 (coef=1.0000) || sm:  5.8431 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:47 - 0:01:32 -     260 -   36.08 sent/s -  1531.70 words/s - cs:  0.2835 (coef=1.0000) || sm:  5.8337 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:51 - 0:01:35 -     270 -   34.51 sent/s -  1549.49 words/s - cs:  0.2826 (coef=1.0000) || sm:  5.9716 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:54 - 0:01:39 -     280 -   33.97 sent/s -  1367.16 words/s - cs:  0.4152 (coef=1.0000) || sm:  5.9118 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:01:58 - 0:01:42 -     290 -   35.30 sent/s -  1596.73 words/s - cs:  0.2804 (coef=1.0000) || sm:  6.0189 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:01 - 0:01:45 -     300 -   37.80 sent/s -  1665.35 words/s - cs:  0.2938 (coef=1.0000) || sm:  5.8679 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:04 - 0:01:49 -     310 -   33.95 sent/s -  1446.50 words/s - cs:  0.3428 (coef=1.0000) || sm:  5.9405 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:08 - 0:01:52 -     320 -   37.97 sent/s -  1511.66 words/s - cs:  0.3633 (coef=1.0000) || sm:  5.6526 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:11 - 0:01:55 -     330 -   36.08 sent/s -  1841.29 words/s - cs:  0.2214 (coef=1.0000) || sm:  5.8120 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:14 - 0:01:59 -     340 -   37.46 sent/s -  1400.12 words/s - cs:  0.3615 (coef=1.0000) || sm:  5.6057 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:18 - 0:02:02 -     350 -   36.10 sent/s -  1877.21 words/s - cs:  0.3600 (coef=1.0000) || sm:  5.9365 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:21 - 0:02:05 -     360 -   34.75 sent/s -  1640.27 words/s - cs:  0.3510 (coef=1.0000) || sm:  6.0555 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:24 - 0:02:09 -     370 -   36.81 sent/s -  1700.34 words/s - cs:  0.2711 (coef=1.0000) || sm:  5.8632 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:28 - 0:02:12 -     380 -   34.92 sent/s -  1566.32 words/s - cs:  0.3443 (coef=1.0000) || sm:  5.9556 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:31 - 0:02:16 -     390 -   35.82 sent/s -  1526.02 words/s - cs:  0.3216 (coef=1.0000) || sm:  5.5955 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:34 - 0:02:19 -     400 -   35.04 sent/s -  1929.96 words/s - cs:  0.2009 (coef=1.0000) || sm:  5.9904 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:38 - 0:02:22 -     410 -   34.17 sent/s -  1609.30 words/s - cs:  0.3069 (coef=1.0000) || sm:  6.0326 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:41 - 0:02:26 -     420 -   36.33 sent/s -  1639.81 words/s - cs:  0.4048 (coef=1.0000) || sm:  5.8789 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:45 - 0:02:30 -     430 -   29.90 sent/s -  1451.43 words/s - cs:  0.2432 (coef=1.0000) || sm:  5.9186 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:49 - 0:02:33 -     440 -   32.55 sent/s -  1565.85 words/s - cs:  0.3051 (coef=1.0000) || sm:  6.1500 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:53 - 0:02:37 -     450 -   33.52 sent/s -  1511.78 words/s - cs:  0.3370 (coef=1.0000) || sm:  5.9842 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:02:56 - 0:02:41 -     460 -   33.60 sent/s -  1650.74 words/s - cs:  0.2768 (coef=1.0000) || sm:  5.9398 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:00 - 0:02:44 -     470 -   35.12 sent/s -  1598.76 words/s - cs:  0.3009 (coef=1.0000) || sm:  5.8150 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:03 - 0:02:47 -     480 -   35.37 sent/s -  1641.27 words/s - cs:  0.3236 (coef=1.0000) || sm:  5.6667 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:06 - 0:02:51 -     490 -   35.84 sent/s -  1627.95 words/s - cs:  0.3632 (coef=1.0000) || sm:  5.8323 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:10 - 0:02:54 -     500 -   34.22 sent/s -  2030.72 words/s - cs:  0.2406 (coef=1.0000) || sm:  5.8774 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:13 - 0:02:58 -     510 -   34.77 sent/s -  1803.81 words/s - cs:  0.2432 (coef=1.0000) || sm:  5.9834 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:16 - 0:03:01 -     520 -   38.14 sent/s -  1729.77 words/s - cs:  0.2700 (coef=1.0000) || sm:  5.9172 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:20 - 0:03:05 -     530 -   32.00 sent/s -  1444.79 words/s - cs:  0.2402 (coef=1.0000) || sm:  5.7917 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:23 - 0:03:08 -     540 -   36.28 sent/s -  1625.21 words/s - cs:  0.2740 (coef=1.0000) || sm:  5.7773 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:27 - 0:03:11 -     550 -   35.46 sent/s -  1724.69 words/s - cs:  0.2427 (coef=1.0000) || sm:  5.7769 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:30 - 0:03:15 -     560 -   35.35 sent/s -  1407.49 words/s - cs:  0.4093 (coef=1.0000) || sm:  5.7043 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:34 - 0:03:18 -     570 -   35.61 sent/s -  1568.18 words/s - cs:  0.3423 (coef=1.0000) || sm:  5.9312 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:37 - 0:03:21 -     580 -   35.13 sent/s -  1820.23 words/s - cs:  0.3282 (coef=1.0000) || sm:  5.8401 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:40 - 0:03:25 -     590 -   34.15 sent/s -  1746.69 words/s - cs:  0.2816 (coef=1.0000) || sm:  5.8540 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:44 - 0:03:28 -     600 -   34.85 sent/s -  1694.99 words/s - cs:  0.2752 (coef=1.0000) || sm:  5.9111 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:47 - 0:03:32 -     610 -   38.23 sent/s -  1552.71 words/s - cs:  0.3170 (coef=1.0000) || sm:  5.5816 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:51 - 0:03:35 -     620 -   33.60 sent/s -  1598.11 words/s - cs:  0.2975 (coef=1.0000) || sm:  5.7904 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:54 - 0:03:39 -     630 -   32.62 sent/s -  1333.77 words/s - cs:  0.3639 (coef=1.0000) || sm:  5.7078 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:03:58 - 0:03:42 -     640 -   33.65 sent/s -  1597.42 words/s - cs:  0.2659 (coef=1.0000) || sm:  5.7019 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:01 - 0:03:45 -     650 -   38.70 sent/s -  1374.33 words/s - cs:  0.3942 (coef=1.0000) || sm:  5.6559 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:04 - 0:03:49 -     660 -   36.89 sent/s -  1677.09 words/s - cs:  0.2673 (coef=1.0000) || sm:  5.8240 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:07 - 0:03:52 -     670 -   38.48 sent/s -  1513.09 words/s - cs:  0.2227 (coef=1.0000) || sm:  5.4471 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:11 - 0:03:55 -     680 -   33.58 sent/s -  1748.18 words/s - cs:  0.2358 (coef=1.0000) || sm:  6.0286 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:15 - 0:03:59 -     690 -   31.41 sent/s -  1585.95 words/s - cs:  0.2727 (coef=1.0000) || sm:  5.9826 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:18 - 0:04:03 -     700 -   33.83 sent/s -  1568.21 words/s - cs:  0.3249 (coef=1.0000) || sm:  5.9631 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:22 - 0:04:06 -     710 -   34.90 sent/s -  1377.03 words/s - cs:  0.3270 (coef=1.0000) || sm:  5.7796 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:25 - 0:04:09 -     720 -   36.75 sent/s -  1812.62 words/s - cs:  0.2588 (coef=1.0000) || sm:  5.7848 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:28 - 0:04:13 -     730 -   38.45 sent/s -  1510.73 words/s - cs:  0.3812 (coef=1.0000) || sm:  5.7470 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:32 - 0:04:16 -     740 -   33.45 sent/s -  1562.72 words/s - cs:  0.3093 (coef=1.0000) || sm:  6.0551 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:35 - 0:04:19 -     750 -   38.07 sent/s -  1755.45 words/s - cs:  0.3156 (coef=1.0000) || sm:  5.7563 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:38 - 0:04:23 -     760 -   35.19 sent/s -  1591.58 words/s - cs:  0.3285 (coef=1.0000) || sm:  5.9336 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:42 - 0:04:26 -     770 -   35.79 sent/s -  1489.94 words/s - cs:  0.3285 (coef=1.0000) || sm:  5.8216 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:45 - 0:04:30 -     780 -   32.45 sent/s -  1660.84 words/s - cs:  0.2692 (coef=1.0000) || sm:  6.0507 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:49 - 0:04:33 -     790 -   36.41 sent/s -  1847.91 words/s - cs:  0.3290 (coef=1.0000) || sm:  5.9017 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:52 - 0:04:36 -     800 -   36.76 sent/s -  1570.78 words/s - cs:  0.3270 (coef=1.0000) || sm:  5.8777 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:55 - 0:04:40 -     810 -   36.44 sent/s -  1906.89 words/s - cs:  0.2940 (coef=1.0000) || sm:  5.7170 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:04:58 - 0:04:43 -     820 -   35.86 sent/s -  1789.21 words/s - cs:  0.2793 (coef=1.0000) || sm:  5.9858 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:02 - 0:04:46 -     830 -   34.21 sent/s -  1690.98 words/s - cs:  0.2040 (coef=1.0000) || sm:  6.0058 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:05 - 0:04:50 -     840 -   36.58 sent/s -  1718.78 words/s - cs:  0.2778 (coef=1.0000) || sm:  5.8042 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:09 - 0:04:53 -     850 -   35.05 sent/s -  1433.00 words/s - cs:  0.3537 (coef=1.0000) || sm:  5.9138 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:12 - 0:04:57 -     860 -   34.18 sent/s -  1529.07 words/s - cs:  0.3380 (coef=1.0000) || sm:  5.8758 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:15 - 0:05:00 -     870 -   38.83 sent/s -  1712.97 words/s - cs:  0.2064 (coef=1.0000) || sm:  5.5541 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:18 - 0:05:03 -     880 -   39.26 sent/s -  1401.88 words/s - cs:  0.3852 (coef=1.0000) || sm:  5.6786 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:21 - 0:05:06 -     890 -   38.60 sent/s -  1483.64 words/s - cs:  0.2780 (coef=1.0000) || sm:  5.3995 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:25 - 0:05:09 -     900 -   34.63 sent/s -  1509.60 words/s - cs:  0.3298 (coef=1.0000) || sm:  6.0215 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:28 - 0:05:13 -     910 -   38.56 sent/s -  1398.70 words/s - cs:  0.4526 (coef=1.0000) || sm:  5.6257 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:31 - 0:05:16 -     920 -   36.20 sent/s -  1834.69 words/s - cs:  0.1861 (coef=1.0000) || sm:  5.8788 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:34 - 0:05:19 -     930 -   38.90 sent/s -  1635.29 words/s - cs:  0.3267 (coef=1.0000) || sm:  5.5017 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:38 - 0:05:22 -     940 -   38.61 sent/s -  1748.51 words/s - cs:  0.2041 (coef=1.0000) || sm:  5.6268 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:41 - 0:05:25 -     950 -   36.67 sent/s -  1514.58 words/s - cs:  0.3357 (coef=1.0000) || sm:  5.5373 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:41 - 0:05:26 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 16:05:41 - 0:05:26 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 16:05:44 - 0:05:28 -     960 -   39.11 sent/s -  1383.29 words/s - cs:  0.3692 (coef=1.0000) || sm:  5.6237 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:47 - 0:05:32 -     970 -   36.95 sent/s -  1641.61 words/s - cs:  0.2724 (coef=1.0000) || sm:  5.6196 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:50 - 0:05:35 -     980 -   35.79 sent/s -  1474.09 words/s - cs:  0.3635 (coef=1.0000) || sm:  5.7758 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:54 - 0:05:38 -     990 -   38.65 sent/s -  1450.49 words/s - cs:  0.3209 (coef=1.0000) || sm:  5.5418 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:57 - 0:05:41 -    1000 -   36.51 sent/s -  1557.89 words/s - cs:  0.2805 (coef=1.0000) || sm:  5.7740 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 16:05:57 - 0:05:41 - ============ End of epoch 0 ============
INFO - 12/20/23 17:53:06 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 17:53:06 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 17:53:06 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 17:53:06 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 17:53:06 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 17:53:06 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 17:53:06 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 17:53:06 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 17:53:06 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 17:53:06 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 17:53:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:53:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:53:06 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 17:53:06 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 17:53:06 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 17:53:06 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:53:06 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 17:53:07 - 0:00:02 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(1500, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 17:53:07 - 0:00:02 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 17:53:07 - 0:00:02 - Number of parameters (encoder): 11331201
INFO - 12/20/23 17:53:07 - 0:00:02 - Number of parameters (decoder): 31278014
INFO - 12/20/23 17:53:08 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 17:53:08 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 17:53:08 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 17:54:35 - 0:00:00 - ============ Initialized logger ============
INFO - 12/20/23 17:54:35 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments/chart2text/run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 12/20/23 17:54:35 - 0:00:00 - The experiment will be stored in experiments/chart2text/run1
                                     
INFO - 12/20/23 17:54:35 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 12/20/23 17:54:35 - 0:00:00 - ============ Data summary ============
INFO - 12/20/23 17:54:35 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 17:54:35 - 0:00:00 - Removed 0 empty sentences.

INFO - 12/20/23 17:54:35 - 0:00:00 - Content-Selection Data -       5703
INFO - 12/20/23 17:54:35 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 12/20/23 17:54:35 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 12/20/23 17:54:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:54:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:54:35 - 0:00:00 - Para Data          -       5703
INFO - 12/20/23 17:54:35 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 12/20/23 17:54:35 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 12/20/23 17:54:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 12/20/23 17:54:35 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 12/20/23 17:54:37 - 0:00:01 - Encoder: TransformerEncoder(
                                        (position_embeddings): Embedding(1500, 512)
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 12/20/23 17:54:37 - 0:00:01 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0-5): 6 x TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0-5): 6 x LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0-5): 6 x MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 12/20/23 17:54:37 - 0:00:01 - Number of parameters (encoder): 11331201
INFO - 12/20/23 17:54:37 - 0:00:01 - Number of parameters (decoder): 31278014
INFO - 12/20/23 17:54:37 - 0:00:02 - ============ Starting epoch 0 ... ============
INFO - 12/20/23 17:54:37 - 0:00:02 - Creating new training dataOld iterator (cs) ...
INFO - 12/20/23 17:54:37 - 0:00:02 - Creating new training dataOld iterator (sm) ...
INFO - 12/20/23 17:54:41 - 0:00:06 -      10 -   31.85 sent/s -  1347.67 words/s - cs:  0.7875 (coef=1.0000) || sm:  7.4146 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:54:45 - 0:00:09 -      20 -   33.71 sent/s -  1500.27 words/s - cs:  0.3767 (coef=1.0000) || sm:  6.1587 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:54:48 - 0:00:13 -      30 -   34.49 sent/s -  1511.03 words/s - cs:  0.3554 (coef=1.0000) || sm:  6.1936 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:54:53 - 0:00:18 -      40 -   25.18 sent/s -  1270.24 words/s - cs:  0.4525 (coef=1.0000) || sm:  6.1982 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:54:56 - 0:00:21 -      50 -   36.02 sent/s -  1507.23 words/s - cs:  0.3418 (coef=1.0000) || sm:  6.1299 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:54:59 - 0:00:24 -      60 -   35.14 sent/s -  1878.79 words/s - cs:  0.2868 (coef=1.0000) || sm:  6.1567 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:55:03 - 0:00:28 -      70 -   34.43 sent/s -  1747.23 words/s - cs:  0.3017 (coef=1.0000) || sm:  6.1403 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:55:06 - 0:00:31 -      80 -   34.87 sent/s -  1763.06 words/s - cs:  0.3172 (coef=1.0000) || sm:  6.0250 (coef=1.0000) - Transformer LR = 1.0000e-03
INFO - 12/20/23 17:55:10 - 0:00:34 -      90 -   36.47 sent/s -  2020.88 words/s - cs:  0.2711 (coef=1.0000) || sm:  6.0118 (coef=1.0000) - Transformer LR = 1.0000e-03
